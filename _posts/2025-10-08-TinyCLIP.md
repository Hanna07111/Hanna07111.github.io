---
title: "TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance"
date: 2025-10-08
categories: [Capstone, 논문리뷰]
tags: [Knowledge Distillation, CLIP]
---

[TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance](https://arxiv.org/abs/2309.12314)


## Abstract

- Novel Cross-modal distillation method
    - Affinity mimicking
    - Weight inheritance
- Multi-stage progressive distillation
- Comprehensive experiments

## 1. Introduction

- Large-scale language-image pretrained model → high cost of storage, memory, computation time..
- Distillation of language-image Cross-modal model이 해결해야할 챌린지
    - CLIP류의 모델은 image encoder, text encoder 두 브랜치로 이루어져 있음 → 모달리티 간의 상호작용을 고려하는 것이 중요
    - Cost, 컴퓨팅 자원의 문제
- 위 두 챌린지를 해결하기 위한 방법 ⇒ TinyCLIP
    - Affinity mimicking
    - Weight inheritance
- Affinity mimicking
    - 이미지 피처, 텍스트 피처가 아닌 ‘image-text affinity space’의 지식을 전달
    - 이미지, 텍스트 임베딩 간의 코사인 유사도를 학생 모델에 전달
    - 학생 모델이 교사 모델의 이미지-언어 피처 정렬방식을 배우게 함
- Weight inheritance
    - Pretrain 된 가중치를 학생 모델에 전달
    - 어떤 가중치를 전달해줘야 하는지가 문제
        
        → Manual and automatic inheritance
        
    - manual → 직접 중요한 가중치를 선택하는 것만으로도 좋은 효과
    - Automatic → learnable mask 도입
- weight inheritance 확장 → multi-stage progressive procedure
    - 여러 단계를 거쳐 중요한 파라미터를 상속 시키는 방식
    - 교사 모델의 성능이 높고, 교사-학생 모델 간 구조가 유사해야, 더 좋은 성능을 보임
    - 가중치 상속을 여러 단계로 나누어, 교사 모델의 가중치를 점진적으로 상속 받을 수 있게 함
- Experiment
    - 속도, 모델 크기와 상관없이, imageNet zero-shot 평가와, 다양한 downstream task에서 좋은 성능을 보임
    (모델이 작거나 속도를 높여도, CLIP 수준의 zero-shot 인식 능력을 유지하고, 여러 downstream task에서 경쟁적인 성능을 보임)
- Contribution
    - New cross-modal distillation approach
    - 소형 규모의 SOTA language-image pre-trained model을 제안, 속도와 정확도 사이에서 최적점을 찾음

## 2. Related Work

**Language-Image Pre-training**

- contrastive language-image pre-training ⇒ 인상적인 제로샷 성능과 일반화 능력을 보여줌 (ex.CLIP)
- 사전학습된 langue-image model을 Scale-up 하고자 하는 노력들은 많았으나, scale down 하고자 하는 시도는 적음 ⇒ 있더라도 특정 task에 집중한 모습
- CLIP compression을 시도한 최초의 연구

**Knowledge Distillation**

- 지식을 큰 모델에서 작은 모델에서 전달하는 방식
- 단일 모달리티에 지식 증류를 적용하는 방식은 광범위하게 연구됨
- Cross-modality에 적용하는 시도는 상대적으로 부족 → 있더라도 특정 태스크에만 초점을 둠, 일반적인 downstream task에 적용될 수 있는 방법을 제한
- 이 논문에서는 일반적인 cross-modal 사전 학습을 증류하는 데에 초점 ⇒ 증류된 모델(학생 모델)이 여러 태스크에 전이될 수 있도록하기 위함

**Weight inheritance**

- 기존의 pruning technique과 비슷
- 기존 방식과의 차이점
    - 기존 → 단일 모달에 초점 / 현 연구 → 다중 모달리티에 초점, 각각의 모달리티의 중복성 특성이 다름, 이러한 차이를 고려해 각각의 모달리티에서 중요한 가중치를 선택
    - 기존 → pruned architecture (가지치기 한 후의 ‘구조’) 의 중요성을 강조 / 현 연구 → original model에서 에서 오는 가중치가 특히 CLIP을 압축하는 문제에서는 매우 중요하다고 주장
    - 현 연구 → Progressive multi-stage process 사용, 작은 모델의 훈련 수렴을 가속화시키는 데에 도움

## 3. Method

### 3.1 Distillation with Affinity Mimicking

- CLIP류 language-image pre-training model의 특징
    - Two branch → image encoder, text encoder로 이루어짐
    - 시각, 텍스트 표현이 contrastive loss를 최소화 시키는 방법을 통해 공통의 공간으로 linearly project 됨
    - 같은 pair일 경우 1, 아닐 경우 0인 identity matrix가 supervision
        
        ⇒ negative pair 간의 섬세한 관계를 학습하지 못하게 함 
        
        ⇒ affinity mimicking 도입, 교사 모델로부터 학생 모델이 negative pair 간의 유사도를 배우가 함
        
- Two type of affinity distillation loss 고려
    - Image-to-language loss
        - Image-to-language affinity를 기반으로 정렬을 배움
        - Image-to-language affinity → 모든 text description에 대한 image의 유사도
    - Language-to-image loss
        - Language-to-image affinity를 활용
        - Language-to-image → 모든 image description에 대한 text description의 유사도
    - 위 두 loss를 모두 고려하여 distillation loss를 formulate (CE는 cross-entropy loss, s는 student, t는 teacher)
- 수식
    
    ![distillation loss](/assets/img/tinyclip1.png)
    
    ![distillation loss](assets/img/tinyclip2.png)
    
- Affinity mimicking → student model이 large model의 visual-linguistic 정렬 방식을 배우게 함 (두 모달리티의 ‘피처’가 아닌 ‘관계에 중점을 둠)

### 3.2. Distillation with Weight Inheritance

- Original CLIP → 학습 시키는 데 오래걸림
    
    ⇒ weight inheritance 도입
    
    ⇒ 어떤 weight이 중요?
    
    ⇒ 두 가지 방법 시도 및 제안
    

**Manual Inheritance**

- Text encoder → more redundancy in depth (layer-wise)
- Image encoder → more redundancy in width (channel-wise)
- Text encoder ⇒ 균등한 간격으로 k개의 layer를 선택
- Image encoder → 앞쪽 k개의 채널을 선택
- 이렇게 직접 선택을 해주는 것만으로도 성능이 좋았음..!

**Automatic Inheritance**

- 어떤 가중치를 전달할 지 결정하기 위해 사전지식이 필요함 → 다양한 모델에 적용하기 어려움
- Learnable mask 도입
- Vision, language branch에 각각 ‘독립적으로’ 적용
- Sparsity constraint 도입 → compression requirement에 맞추도록
- Transformer 구조를 예시로 설명
    - MHA의 불필요한 head 와 FFN의 불필요한 뉴런을 찾기 위해 mhead, mint 도입
    - mhead → imposed on the activation of attention heads ⇒ Multi-head-attention의 각 헤드의 결과값에 곱해짐
    - mint → imposed on the intermediate layer of FFN → FFN의 중간 뉴런들에 곱해짐
    - membed → shared across all layers
        
        → 모든 layer에 공유 (MHA 이후, FFN 이후) → residual connection으로 차원이 서로 연결되므로, 일관성 유지를 위해 모든 층이 mask를 공유해야 함
        
- Loss
    - L = Ldistill + Lsparsity
    - 교사모델의 유사성 공간도 학습하면서 + 모델이 자연스럽게 압축될 수 있게끔 유도하는 loss
    - Lsparsity → 모델의 압축률 p를 목표하는 압축률 q로 수렴하도록 하는 loss
    - p → 모델에 남아 있는 가중치 비율 (압축 비율)
    - Sparsity = 1-p (사라진 가중치의 비율)

### 3.3 Progressive Multi-Stage Distillation

- High target sparsity를 얻고자 하는 시도 → accuracy의 극심한 감소와, convergence failure를 일으킬 수 있음
- 갑자기 많은 가중치를 버리면, 중요한 가중치도 버리게 될 수 있음
    
    ⇒ multi-stage progressive distillation 제안
    
    ⇒ 매 단계마다 25% 정도씩만 압축
    
- 단계별 과정
    - Distillation loss와 sparsity loss를 활용해 모델 압축
    - 압축된 모델과 distillation loss로 cross-modal distillation 진행
    - 목표 압축률에 도달할 때까지 위 과정을 반복

## 4. Experiments

### 4.1 Implementation Details

**Architecture**

- CLIP
    - Weight inheritance
        - OpenCLIP ViT-B/32 or ViT-B/16 & CLIP ResNet-50
        - Pre-trained on LAION-2B, WIT-400M respectively
    - distillation
        - OpenCLIP ViT-B/32 pre-trained on LAION-2B→ teacher model
- DaViT
    - Student model → Florence-DaViT-5M on LAION-400M
    - Teacher model → Florence-DaViT-D3 on FLD-900M

**Affinity mimicking**

- 교사 모델-학생 모델 간 interaction scehmem을 여러 가지로 정의
- L0 → contrastive loss
- L1 → Ldistill
- 여러 로스는 서로 합쳐질수도 있으며, 합쳐질 시 가중치는 1
- Temperature parameter = 1/50이 default

**Weight Inheritance**

- OpenCLIP ViT-B, CLIP ResNet-50
- Multi-stage progressive distillation 사용
- 매 단계마다 Manual or automatic weight inheritance 적용
- Manual inheritance
    - Image encoder → width 감소
    - Text encoder → depth 감소
- Automatic inheritance
    - Target sparsity 설정
    - 매 단계마다, trainable mask가 1로 설정되고, 3000 iteration을 통해 업데이트 됨
    - optimizer → AdamW, constant learning rate → 0.01, no weight decay
    - Lambda, beta → 0.01로 초기화

**Training Settings**

- datasets
    - LAION-400M
    - YFCC-15M
- LAION-400M
    - 3단계 압축
    - 100%→75% for 6 epochs
    - 75% → 50% for 16 epochs
    - 50% → 25% for 16 epochs
- YFCC-15M
    - 2단계 압축
    - 100%→ 50% for 25 epochs
    - 50% → 10% for 25 epochs
- CLIP 하이퍼 파라미터 (학습률 제외) → 10^(-4)
- 다른 언급이 없으면 ablation study 도 같은 세팅 (학습률, epoch 수 제외)

**Evaluation Settings**

- Zero-shot transfer evaluation and robustness evaluation
    - Follow same prompt engineering in CLIP
    - ImageNet 데이터에 클래스 당 80 템플릿 적용
- Linear probe → Elevator toolkit 사용
    - Classification head 가 50 epoch으로 학습
- Text embedding의 parameter size는 고려하지 않음
    - Same hidden dimension과 vocabulary size만 가지고 있으면 모델별로 모두 같은 개수를 가지므로, 압축률, 경량화 성능 평가 등등에 포함하지 않음

### 4.2 Comparison with State-of-the-Art Models

- YFCC-15M
    - OpenCLIP ViT-B/16 → TinyCLIP ViT-39M/16
    - ImageNet 제로샷 성능에서 original clip보다 25.9% 성능 증가
    - 더 압축한 모델 (TinyCLIP ViT-8M/16)도 성능 3.5% 증가 (더 적은 파라미터, 학습 시간 더 훨씬 빠름)
- LAION-400M
    - distill 3 different models
    - Florence DaViT → TinyCLIP DaViT-5M
        - 이미지넷 제로샷 성능 5% 상승
        - Image-to-text retreival에서도 성능 상승
    - CLIP ResNet → TinyCLIP-30M
        - 첫번째 단계에서 4 epoch 학습&본 모델에서 75% 파라미터만 상속받음
        - 이미지넷 제로샷 성능은 약간 하락
        - Retrieval task에서는 향상
    - TinyCLIP-30M → TinyCLIP ResNet-19M
        - 30M 모델에서 상속받음
        - 12epoch 학습
        - 추론 속도는 2배 올리고, 파라미터는 절반으로 줆
        - 파라미터가 크게 줄었기 때문에 이미지넷 성능을 2.7% 정도 감소
    - OPenCLIP ViT-B/32
        - Manual 또는 automatic으로 가중치 상속
        - Three-stage progressive distillation
        - Manual → TinyCLIP ViT-61M/32가 OpenCLIP에 맞먹는 성능을 보임
        - Automatic이 좀 더 좋은 결과를 보임
        - YFCC-15M에 고품질의 데이터가 많아서, LAION 데이터와 함께 썼을 때 retrieval의 성능 높아짐

### 4.3 Ablation Study

**Impact of affinity mimicking**

- 서로 다른 interaction mode (Loss) 사용함
- Affinity mimicking의 성능이 가장 좋았음
- Cross modality → 학생의 이미지와 텍스트가 상호작용하지는 않지만, 교사모델의 임베딩 공간에 정렬이 됨
- Single modality interaction은 아예 같은 공간에 정렬하는 것 자체를 실패

**Impact of weight inheritance**

- 가중치 상속을 안 한 것보다 한 게 더 나았음
- 특히 manual 보다는 automatic의 성능이 더 좋음

**The redundancy of language-image models**

- Image encoder&text encoder 중복성 분석
- Similar strategy & layer 제거했을 때 zero-shot accuracy 측정
- Text encoder → layer간 유사도 높음
    
    → layer 간에 출력하는 output의 차이가 크지 않다는 것
    
    ⇒ 성능의 큰 저하 없이 layer 삭제 가능
    
- Image encoder → layer간 유사도가 크지 않음 + layer 제거했을 땐 성능 저하 큰 편
    
    ⇒ layer 제거는 좀 위험
    
- Proposed automatic weight inheritance을 바탕으로 한 redundancy 분석
    - Width analysis
        - Text encoder의 Embedding channel → 아주 적은 제거
        - Image encoder의 embedding channel → 아주 큰 제거
    - Depth analysis
        - Text encoder의 MHA layer와 FFN channel에서의 큰 비율의 삭제가 이루어짐
    - image encoder는 width 방향으로, text encoder는 depth 방향으로 압축되었다는 것을 알 수 있음

**Impact of teacher models**

- 단순히 성능이 좋은 모델이 아니라, 학생 모델과 구조가 유사하면서 성능이 좋은 모델이 가중치 상속에서도 좋은 결과를 보임

**Impact of multi-stage progressive distillation**

- 2단계로 나누어서 가중치 상속을 한 경우, 1단계 가중치 상속 (즉, 바로 압축한) 모델보다 높은 제로샷 성능을 보임
- 점진적 압축이 바로 압축보다 지식 전이에 유리

**Training cost**

- Original → 32 epoch, 287 GPU day 소모
- 75% parameter 모델 → 7.8배 빠른 속도로 OpenCLIP과 비슷한 성능
- 50% parameter 모델 → 1.4배 빠른 속도로 OpenCLIP과 비슷한 성능
- Affinity mimicking + 가중치 상속으로 인한 초기화 ⇒ training benefits
- 특히나 가중치 상속이 증류를 가속화 하는 데 도움이 있었음을 알 수 있음

### 4.4 Transfer Learning Results

- 제로샷 벤치마크 → 7개 데이터셋에서 가장 높은 성능
- Linear-probe 벤치마크 → 9개 데이터셋에서 가장 높은 성능
- Zero-shot robustness evaluation
    - ImageNet distribution이 아닌 5개의 데이터셋에 대한 성능 측정
    - TinyCLIP ViT-63M/32 → OpenCLIP outperform, 3개의 데이터셋에서 성능 CLIP보다 성능이 우수
    - TinyCLIP ViT-45M/32 → 50% 적은 파라미터를 썼지만, IN-R, IN-Sketch에서 CLIP보다 좋은 성능

## 5. Conclusions