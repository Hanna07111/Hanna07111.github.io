---
title: "CLIP-KD: An Empirical Study of CLIP Model Distillation"
date: 2025-10-20
categories: [Capstone, 논문리뷰]
tags: [Knowledge Distillation, CLIP]
---

[CLIP-KD: An Empirical Study of CLIP Model Distillation](https://arxiv.org/abs/2307.12732)

## Abstract

- 다양한 지식 증류 전략을 적용
    - relation, feature, gradient and contrastive paradigms
- Simple feature mimicry with mse가 가장 효과가 좋음
- Interactive contrastive learning across teacher and student encoder도 효과적
- Teacher와 student간 feature 유사도를 최대화시키는 게 핵심(?)

## 1. Introduction

- CLIP의 성능을 높이려는 시도는 많았지만, 작은 CLIP을 발전시키려는 도전은 적었음
- TinyCLIP의 단점
    - 가중치 상속 → teacher model, student model 같은 아키텍처 스타일을 가지고 있어야 함
- Relation, feature, gradient, contrastive 측면의 포괄적인 연구를 진행
- CLIP-KD는 Architectural-cue에 의존하지 않고, 어떤 교사-학생 아키텍처 쌍에든지 일반화할 수 있음
- Distillation을 위한 두 가지 전략 → Mimicry and interaction
    - Mimicry learning → teacher의 지식을 배움 (basic KD)
        - 어떤 지식을 전달?
        - Contrastive image-to-text relationships
        - (Image,text) features
        - Gradients
    - Interactive learning → 상호작용형, joint contrastive learning
        - 학생이 직접 지식을 전달 받는 것이 아닌, 교사로부터 간접적으로 학습하게 함
        - 예시) 학생 임베딩을 앵커로 두고 교사 임베딩의 contrastive loss 최소화 ,student&teacher feature aggregation  ⇒??????…. 두고보자…
- Dataset
    - Train → conceptual dataset
    - 제로샷 성능 측정 → ImageNet
    - Cross-modal retrieval → MSCOCO, Flickr
- 결과
    - 모든 방법이 어느 정도 학생 clip 모델의 성능을 발전시킴
    - Simple feature mimicry with mse의 성능이 가장 좋았음
    - Interactive contrastive learning 이 두 번째
    - 교사와 학생간 피처의 유사도 얼마나 높은지가 중요

## 2. Related Works

**Language-Supervised Learning**

- CLIP → 대조 학습 이용
- ALIGN
- Contrastive multi-modal learning
- Generative approach

**CLIP Variants**

- SLIP - CLIP+visual self-suervised learning
- MaskCLIP - mask self-distillation to train image EMA encoder for CLIP
- DeCLIP → data-efficient pre-training
- FLIP, A-CLIP → accelerate CLIP training 하기 위한 방법

**Multi-Modal Knowledge Distillation**

- DistillVLM → align hidden attention distribution&feature map
- TinyCLIP

## 3. Methodology

### 3.1. A Brief Review of CLIP

**CLIP**

- 이미지-텍스트 쌍을 활용해 같은 pair는 가깝게, 서로 다른 pair는 멀게 하는 contrastive loss를 이용한 사전학습

### 3.2. CLIP Knowledge Distillation

**3.2.1 Contrastive Relational Distillation**

- CRD
- 전달하는 지식 종류 → output-oriented contrastive distribution
    - 대조 분포
    - 이미지와 텍스트의 유사도 분포
- contrastive distribution 수식

![text-to-image도 같은 형식](assets/img/clipkd1.png)

text-to-image도 같은 형식

- teacher-student의 유사도 분포를 아래 KL divergence loss를 활용해 정렬

![image.png](assets/img/clipkd2.png)

[KL-divergence 참고 자료](https://jaylala.tistory.com/entry/%EB%94%A5%EB%9F%AC%EB%8B%9D-with-Python-KL-Divergence-Loss%EB%9E%80)

**3.2.2 Feature Distillation**

- teacher-student의 feature embedding을 정렬
- MSE loss를 활용

![image.png](assets/img/clipkd3.png)

- 만약 두 임베딩의 사이즈가 다르다면, student의 임베딩에 linear projection 적용

**3.2.3 Masked Feature Distillation**

- student 모델의 input으로 masked image 사용
- masked image로부터 뽑아낸 image embedding과 teacher의 image embedding을 정렬시킴
    
    → 모델로 하여금 단순히 teacher의 임베딩을 맞추는 것이 아니라, contextual reasoning을 하게함
    
- MSE loss 활용
    
    ![image.png](assets/img/clipkd4.png)
    

**3.2.4 Gradient Distillation**

- 그래디언트 → input(이미지/텍스트 임베딩)에 따라 어떻게 모델이 변화하는지를 알 수 있게 함
- teacher-student의 gradient를 정렬시킴 (gradient가 서로 일관되게끔 만듦)

⇒ 이를 통해 input에 따라 output이 어떻게 바뀌어야 하는지를 잘 알 수 있게 됨

⇒ 즉, 텍스트/이미지 임베딩에 따라 Loss가 어떻게 바뀌는지 그 ‘방향’을 teacher에게서 학습하게 됨

⇒ teacher와 비슷하게 행동하는 법을 배우게 됨

- *Loss 함수 그 자체가 아닌, ‘Loss 함수의 변화의 방향성’을 배우게 됨*
- MSE loss 활용

![image.png](assets/img/clipkd5.png)

**3.2.5. Interactive Contrastive Learning**

- teacher-student 간 interaction을 활용
- ICL_I→T Loss
    - 학생의 이미지 임베딩을 기준으로 두고, 선생의 텍스트 임베딩과 유사도 계산
- ICL_T→I loss
    - 학생의 텍스트 임베딩을 기준으로 두고, 선생의 이미지 임베딩과 유사도 계산
- 이 방법을 사용하는 이유
    - ICL loss 최대화 == 교사, 학생 네트워크 간의 mutual information의 하한선 최대화
        
        ⇒ 이를 통해 학생 네트워크가 선생 네트워크로부터 더 많은 common knowledge를 배울 수 있게 됨
        
    - Mutual information → 두 정보간의 의존성 정도

**3.2.6 Augmented Feature Distillation**

- 이 방법도 interaction 활용
- Fusion encoder로 학생, 교사 임베딩을 합치고, 이를 활용
    - 학생, 교사 임베딩 concat
    - Concat한 임베딩을 fusion encoder에 통과시킴
    - Fusion encoder → simple linear projection layer
    - 이렇게 합쳐진 임베딩을 clip contrasitve loss에 적용 → 이 loss를 distillation loss로 활용

**3.2.7 Overall Loss of CLIP Distillation**

![image.png](assets/img/clipkd6.png)

## 4. Experiments

### 4.1 Experimental Setup

**Dataset**

- Pre-training → CC3M, CC12M
- CC3M validation set → cross-modal retrieval evaluation 으로 사용
- Zero-shot classification
    - ImageNet and its several variants
- Zero-shot cross-modal image/text retrieval
    - MSCOCO&Flickr

**Evaluation metrics**

- Retrieval performance in K nearest neighbours → Recall@K (R@1)
- Default
    - image classification → top-1 accuracy
    - Image-to-Text and Text-to-Image retrieval → R@1

**Configuration of visual and text encoders**

![image.png](assets/img/clipkd7.png)

**Training details**

- AdamW optimizer, initial learning rate = 0.001, weight decay = 0.1
- Cosine learning rate schedule with linear warm-up for 10K iterations in 32 epochs
- 8 NVIDIA A800 GPUs
- Batch size → 1024, 각 gpu가 128 sample을 담당
- 모든 distillation loss의 가중치를 1로 설정, learnable temperature은 0.07로 초기화
- 다른 학습 세팅은 original clip을 따름

### 4.2. Ablation Study of Distillation Losses

- Individual distillation loss 활용
    - 어떤 loss 든 베이스라인보다는 성능 증가
    - 그 중에선 FD가 가장 높은 성능 향상을 보임
    - MFD는 FD와 비슷한 성능 → clip-kd의 최종 loss set에는 포함하지 않음
    - ICL, CRD 도 제로샷에서 두 번째, 세 번째로 높은 성능
    - GD, AFD는 상대적으로 moderate 한 성능 향상
- Combine loss terms
    - FD+ICL은 둘을 각각 사용한 것보다 성능이 훨씬 향상 → FD와 ICL은 상호보완적
    - FD+ICL에 CRD 추가 → 성능 향상
    - GD나 AFD는 눈에 띄는 성능 향상을 보이지 않음
    - FD+CRD+ICL → best case
    
    ⇒ 이 논문에선 이 조합을 default로 사용
    

### 4.3 Distilling CLIP Models

- zero-shot retrieval & ImageNet classification 성능으로 distillation 성능 평가

**4.3.1 Cross-Modal Retrieval on CC3M**

- Supervised by ViT-B/16
    - I2T retrieval → 2.0~3.9 R@1 gain
    - T2I retrieval → 1.1~3.4 R@1 gain
- Supervised by ResNet-18
    - I2T retrieval → 2.0~4.4
    - T2I retrieval → 1.9~3.8
- ViT, CNN 간의 구조적 차이는 CLIP-KD에서 중요하지 않음
    - Hidden layer의 정보 대신, final output embedding 만을 고려했기 때문

**4.3.2 Zero-Shot Cross-Modal Retrieval** 

- Supervised by ViT-B/16
    - MSCOCO, I2T → 2.1~4.3 gain
    - MSCOCO, T2I → 1.0~3.3 gain
    - Flickr, I2T → 4.9~8.8 gain
    - Flickr T2I → 3.2~6.5 gain
- Supervised by ResNet-101
    - MSCOCO, I2T → 1.8~3.7 gain
    - MSCOCO, T2I → 1.6~3.6 gain
    - Flickr, I2T → 3.3~7.8 gain
    - Flickr T2I → 3.1~7.1 gain

**4.3.3 Zero-Shot ImageNet-Related Classification**

- ImageNet&ImageNet-variants에서의 제로샷 분류 성능 측정
- ViT-B/16 (ImageNet)
    - 모든 학생 모델에서 top-1 accuracy 향상
- ResNet-101
    - 역시 모든 학생 모델에서 top-1 accuracy 향상
    - 향상 정도는 교사 모델이 ViT-B/16이 더 높은듯…?
- 다른 imageNet variants set에서도 성능 향상 확인할 수 있음
- Swin-T는 teacher 모델의 성능도 surpass → 두 가지 이유
    - Swin-T 자체가 powerful 모델
    - CLIP-KD가 적절한 지식을 전달

**4.3.4 Transferred from Laion-400M**

**Cross-dataset comparison**

- Laion-400M으로 사전학습한 모델을 teacher 모델로 사용
    - 학생모델 학습, 즉 지식증류에는 CC3M+12M 사용
- 교사를 CC3M+12M으로 학습시켰을 때보다 학생모델의 성능 향상
    
    ⇒ CLIP-KD가 작은 데이터셋을 학습되는 모델에 대규모 데이터셋에서의 지식을 효과적으로 전달할 수 있음을 의미
    
    ⇒ 이 방식으로 대규모 데이터셋에서의 지식을 너무 많은 데이터셋으로 학습하지 않고도 배울 수 있음 
    

**Impact of teacher models with different sizes**

- 서로 다른 두 크기의 모델을 교사 모델로 사용
- 더 큰 모델을 교사로 쓰는 게 성능이 더 안 좋음
    - 아마 교사 모델과 학생 모델 간에 용량 차이가 학생 모델을 교사 모델과 정렬하기 어렵게 만들었을 수도
        
        → future research의 주제가 될 수 있음
        
        → 큰 교사 모델의 임베딩에 지식 응축 레이어를 써서 비교해보면.. 안 되남.. 
        

**Comparison with TinyCLIP**

- TinyCLIP보다 좋은 성능을 보임

### 4.4 Analysis

- teacher → ViT-B/16, student → ViT-T/16

**Training curve of CLIP-KD**

(1) Training loss analysis

- Baseline task loss > CLIP-KD task loss
- Task loss > ICL loss → 이미 잘 정렬된 교사 모델의 임베딩을 가지고 contrastive loss를 계산하기 때문

(2) Sample similarity analysis

- Relative distance between positive and negative pairs
- Baseline&CLIP-KD → 둘 다 증가하는 그래프
- CLIP-KD의 pair similarity(relative distance)가 대체로 더 높음

**Interpreting why various KD methods have different performance**

- 증류 후의 교사&학생 feature의 유사도 비교
    - 정확도와 feature의 유사도가 대체로 같이 감, 즉 유사도가 높을수록 학생과 교사 사이의 성능 차이가 줄어듦
    - 같은 이유로 simple FD가 가장 성능이 좋음 (loss를 combine하지 않는다고 했을 때)
    - 다만 FD는 contrastive image-text relation을 고려하지 않음, 이 측면에서는 ICL이 우세하므로, ICL도 높은 유사도를 보임
    - 반면 CRD, GD, AFD는 상대적으로 낮은 유사도와 낮은 정확도를 보임
    - FD+ICL → 이 조합이 피처 정렬과 대조적 증류가 가능하게 하고, 높은 성능 향상을 보임

## 5. Conclusion