---
title: "Learning Transferable Visual Models From Natural Language Supervision"
date: 2025-09-26
categories: [Capstone, 논문리뷰]
tags: [CLIP]
---

[Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)

## Abstract

- fixed set of predetermined object category를 예측하도록 훈련하는 것의 한계점
    
    → 일반화 능력을 제한함
    
    ⇒ learning directly from raw text about image
    
- Pretraining task → 어떤 캡션이 어떤 이미지와 맞는지 예측하는 것만으로도 인터넷 상의 scratch 데이터에서 강력한 이미지 표현을 배울 수 있음
- 사전학습 이후에는 자연어가 시각적 개념을 불러오는 데 쓰임 → zero-shot 가능하게 함
- 대부분의 태스크에 잘 전이 됐고, 추가적인 데이터셋 없이도 fully supervised baseline 모델에 경쟁적

## 1. Introduction and Motivating Work

- 태스크 비의존적 목적함수 + 대규모의 스케일링
    
    → steadily improve capability
    
    ⇒ text-to-text에서 zero-shot transfer to downstream 가능하게 함
    
    → 즉 태스크가 달라졌을 때 추가적인 분류기나, 데이터셋에 맞는 커스텀마이제이션이 없어도 뛰어난 성능을 보임
    
    → 웹에서 수집한 텍스트 데이터가, 고품질로 라벨링된 NLP 데이터보다 낫다..
    
    → 비전에도 적용해볼 수 있나?
    

⇒ 스크래치 텍스트 데이터로 태스크 비의존적 목적함수를 모델에 학습시키는 게 전이학습이 용이한 언어모델을 만드는 데 효과가 있었고, 이를 비전에 시도하고자 함

⇒ 그니까 얘를 비전 모델에 접목하면, 이미지를 텍스트로 적절히 표현할 수 있는 + 전이학습도 잘하는 모델을 만들 수 있지 않을까? 이런 느낌쓰..

⇒ 그니까 비전모델에 대해 기존의 고정된 라벨말고, 대규모 언어모델처럼 그냥 스크래치 데이터를 줘보자 그럼 언어모델에서 텍스트 표현을 배운 것처럼, 이미지 표현을 배울 수도? 

- Prior work
    - 텍스트를 통해 이미지를 표현하고자 하는 시도가 이전에도 많았음 (텍스트를 통해 이미지 표현을 배우고자 하는 시도..)
    - 이미지-텍스트 매핑 시도
    - 가중치 공간에 manifold learning 시도 -?
    - Low-level image&text tag에 멀티모달 모델 얹기
    - CNN, AlexNet 사용 → ImageNet-based pre-training on transfer work와 비슷한 성능
    - N-gram(구) 예측
    - 최근 접근 → transformer-based language modeling, masked language modeling, contrastive objective 활용
- 그럼에도 자연어가 supervision인 연구는 그간 찾아보기 힘듦 → 성능이 좋지 않았음
    - 그래도 narrowly scoped but well-targeted use of weak supervision이 성능 향상에 효과가 있다는 것은 이후 몇 가지 논문에서 증명
        
        (Ex. Instagram 이미지에서 해시태그를 예측해낸다던가, 라벨링에 노이즈가 있는 데이터셋을 활용한다던가.. 하는 그런 태스크)
        
    
    ⇒ 이 연구들이 어쨌든 완전 fix된 데이터와 raw 데이터 사이 중간지점 같은 느낌
    
    ⇒ 하지만 두 연구들 모두 제한된 클래스와 softmax classifier를 사용함 
    
    → 다양한 아웃풋 제한, zero-shot 능력 제한
    
- 그동안의 연구와 이 연구의 가장 큰 차이는 스케일
    - 400 million pair 데이터셋 구축
    - 모델은 convirt의 단순화 버전을 스크래치를 통해 훈련시킨 clip을 사용
    - 스케일이 커질수록 전이학습 성능도 올라감
    
     ⇒ 이 clip은 여러 태스크에 잘 작동하고, zero shot task에 강건하고, 

<details markdown="1">
  <summary>Introduction 정리</summary>
    
    그 동안 사람들은 정해진 라벨로 비전 모델을 학습 ⇒ 일반화, 전이학습 어려움 (새로운 데이터에서 새로운 클래스가 생기면 다시 학습 시켜야 하니까) 
    
    하지만 대규모 언어모델이 그랬던 것처럼 고정된 라벨이 아닌 웹에서 가져온 raw한 자연어를 사용한다면? 일반화가 가능하고 전이학습에 용이해지지 않을까?
    
    대규모 언어모델도 raw한 자연어를 이용해 태스크 의존적이지 않은 사전 학습을 거쳐 뛰어난 텍스트 표현을 배울 수 있었음
    
    그럼 웹에서 가져온 raw한 자연어를 이용하면 뛰어난 이미지 표현도 배울 수 있지 않을까.. 이미지의 의미를 좀 더 잘 담을 수 있게 되지 않을까.. 
    
    비슷한 여러 시도들이 (텍스트를 통해서 이미지 표현을 배우려는 시도, 이미지가 다양한 텍스트를 통해 표현될 수 있도록 하는 시도 → 기존에는 이미지 표현이 강아지, 고양이 이렇게 일대일로 매핑됐다면, 이미지 표현이 강아지, 털, 갈색 등의 다양한 의미를 담을 수 있게 하는 시도) 있었으나, 여전히 좋은 성능을 보이지 못했고, 어찌됐든 대부분 라벨과 소프트맥스를 활용하는 기존의 방식에서 벗어나지 못함
    
    그래서 스케일을 키우고, 대규모 자연어 스크래치로 모델을 학습시킴 ⇒ 일반화에 강하고, 전이학습 및 zero-shot 태스크에 강건한 모델 탄생 
    
    ⇒ CLIP
    
    찐 한줄 정리: natural language, 특히 웹에서 가져온 대규모 text 데이터로 이미지 표현을 배워서, 일반화에 강하고 전이학습에 용이한 모델을 만들고자 함 → 기존의 것들은 한계가 있었지만, 이를 극복한 것이 clip
</details>
    

## 2 Approach

### 2.1 Natural Language Supervision

- Natural language supervision의 장점
    - Scale이 쉬움 → annotation이 따로 필요 없으니까
    - Representation을 배우는 것 뿐 아니라, text의 representation과도 연결
        
        → 제로샷 전이를 용이하게 함
        

### 2.2 Creating a Sufficiently Large Dataset

- 기존의 데이터셋
    - MS-COCO, Visual Genome → 양이 너무 적음
    - YFCC100M → 이미지의 메타 데이터의 퀄리티가 천차만별 → 괜찮은 것만 필터링하면 15million 밖에 안 남음
- 400 million 이미지-쌍 데이터를 직접 수집
    - 다양한 개념을 포함하기 위해, 500,000개의 query를 하나 이상 포함 하는 텍스트-이미지 쌍을 모음
    
    ⇒ WebImageText (WIT)
    

### 2.3 Selecting an Efficient Pre-Training Method

- 그동안의 비전 모델 → 많은 resource 차지 ⇒ Efficienty가 핵심
- 초기 시도 → 이미지 캡션을 예측
    - 효율성 문제
    - 캡션을 예측한다는 게 쉽지 않은 태스크.. description, comment, 등이 너무 다양하니까
- 최근 연구들이 predictive objective 보다 contrastive objective보다 훨씬 표현을 잘 학습한다고 밝힘 + generative 모델보다 훨씬 효율적이라는 것도 알게됨

⇒ 어떤 텍스트가 어떤 이미지와 어울리는지를 예측 

- Contrastive objective
    - NxN의 pair 중 어느 것이 실제 페언지 맞추는 태스크
    - 이를 통해 같은 페어의 코사인 유사도는 크게, 다른 페어의 코사인 유사도는 작도록 하는 multi-modal embedding space를 배우게 됨
    - 이 유사도는 symmetric cross entropy로 최적화함
- 그 외 사전학습 특이사항
    - Image encoder나 text encoder를 기존에 있는 가중치로 초기화하지 않음
    - Representation 과 contrastive embedding space 간 non-linear projection 사용하지 않음 (linear projection 만 사용)
        
        → 두 방식 간에 차이 없었음
        
        → non-linear projection은 현재의 image only self supervised representation learning method에서 공동적응하는데, 현재 clip은 텍스트와 이미지를 둘 다 다루기에, 효과가 없을 것이라 예측
        
    - Remove text transformation function
        - 원래는 여러 텍스트 중 하나를 뽑는 과정 → 여기는 텍스트가 모두 한 문장이라 필요없음
    - Simplify image transformation function
        - 데이터 증강을 위한 이미지 변환 함수 단순화
    - 소프트맥스의 Temperature는 바로 최적화될 수 있게 함   (하이퍼 파라미터로 쓰이지 x)

### 2.4 Choosing and Scaling a Model

- image encoder
    - ResNet-50
        - Antialiased rect-2 blur pooling 적용
        - Replace global average pooling layer → attention pooling mechanism
        - Query - global average-pooled representation of the image
    - ViT
        - 약간의 modification만 적용
        - additional layer normalization to the combined patch and position embeddings before the transformer
- Text encoder
    - Transformer
        - 63M-parameter, 12-layer, 512-wide model with 8 attention heads
        - 문장을 소문자화→ BPE로 토큰화
        - Max sequence length → 76
        - [EOS] 토큰이 feature representation으로 이용됨
        
        (Bert의 CLS 토큰 같은 역할이라고 함 → 다른 토큰들과의 self attention을 통해 모든 문장에 대한 정보를 CLS 토큰이 가져가게 됨, 이와 같은 원리)
        
        - 이렇게 구해진 feature representation을 layer normalize & linearly project 하여 멀티모달 임베딩 공간으로 보냄
        - Masked self attention 사용 → 혹시나 나중에 pre-trained language model 가중치로 초기화하거나, 보조 과제로 언어모델링을 추가할 수 있도록 하기 위해서 → 이 연구를 실제로 진행하지는 않음

[Bert CLS 토큰 관련 참고자료](https://seungseop.tistory.com/m/35)

- Scaling
    - 모델의 깊이/너비/입력 해상도를 같은 비율로 높임
    - Text encoder는 너비만 늘림 → clip의 성능은 텍스트 인코더의 용량에 민감하지 않았음

### 2.5 Training

- 5 ResNet & 3 ViT
    - ResNet-50, ResNet-101, RN50x4, RN50x16, RN50x64
    - ViT-B/32, ViT-B/16, ViT-L/14
- 32 epoch, AdamW를 gain(scaling 하는 층)이나 bias가 아닌 항에 모두 사용
- 코사인 스케쥴에 맞춰 학습률 decay
- initial hyper parameter → Resnet50(baseline) 1번 epoch에 대해 grid search, random search, manual tuning 적용해서 구함
    - 더 큰 모델을 학습할 때는 계산 자원의 문제로, 경험적으로 조정
- Learnable temperature → 0.07로 초기화
- Logit이 100이 넘어가면 클립
- 미니 배치 사이즈 → 32,768 (매우 큼)
    - 메모리 절약을 위해 아래 방법들을 사용
    - Mixed precision 사용 → fp32(32-bit float)대신 fp16(16-bit float)을 섞어서 씀 → 민감한 연산에서만 fp32 사용
    - Gradient checkpointing → 활성값들을 체크포인트에서만 저장, 역전파 시 필요할 때 다시 계산하느 방식
    - Half-precision adam statistics → adam optimizer의 1차, 2차 모멘트를 fp 16으로 저장
    - Half precision stochastically rounded text encoder weights → 텍스트 인코더의 가중치를 확률적 반올림을 적용한 fp 16으로 저장
- 임베딩 유사도 계산
    - 각 gpu가 자신 배치의 임베딩간 유사도만 계산
- Vit-L/14 모델에 336 해상도 이미지로 한 epoch 더 학습 시킴
    - 별다른 얘기가 없을시, 이 논문에서 얘기하는 CLIP은 가장 성능이 좋았던 이 모델을 사용한 것

## 3 Experiments

### 3.1 Zero-shot Transfer

**3.1.1 Motivation** 

- 기존의 일반화 → unseen object category를 탐지할 수 있는지
- Clip에서 사용한 일반화 → unseen dataset에 대한 일반화 → 새로운 데이터셋에도 잘 작동을 하는지
- 많은 분야에서 ‘representation learning’ 능력을 중요하게 생각했지만, 여기서는 ‘task learning’ 능력을 중요하게 생각함
    
    ⇒ 단순히 좋은 임베딩을 출력하는 법을 배우는 게 아니라, 새로운 task에 잘 적응할 수 있는가? 를 보는 것 
    
    ⇒ 이런 시각이라면 데이터셋에 대한 성능을 보는 게 특정 분포에 대한 모델의 성능을 평가할 수 있을 것
    
- 하지만 데이터셋 중 태스크가 분명하지 않은 것들이 있음 → 이럴 경우 분포 변화와 도메인 일반화에 대한 강건성을 평가한다고 보는 게 조금 더 타당
    - 태스크 기반 데이터셋 → 태스크 일반화 능력
    - 벤치마크 데이터셋 → 도메인 일반화 능력
- 이런 식의 zero-shot transfer을 시도했던 게 Visual N-Grams
    - 주어진 이미지에 해단하는 단어 구의 확률을 최대화 시키는 방식으로 학습 → 새로운 데이터셋에 대해서는 클래스의 이름을 구로 바꾸고, 확률이 가장 높은 클래스를 반환
- 태스크 일반화 평가를 위한 zero-shot transfer에 대한 연구는 nlp 분야에서 영감을 받음
    - Gpt-1 도 처음엔 지도 학습 파인튜닝을 개선하기 위한 전이 학습 방법으로 쓰였지만, 이젠 gpt-2 도 여러 태스크를 학습할 수 있는 능력에 대해 초점을 맞추고 있움

**3.1.2 Using CLIP for Zero-shot Transfer**

- 가장 유력한 이미지-텍스트 쌍을 찾았던 것
    
    → 이 능력을 제로샷 전이에도 똑같이 사용
    
- 전체 과정
    1. 이미지와 텍스트를 각각의 인코더로 임베딩 생성
    2. T 로 스케일 된 코사인 유사도 계산, softmax로 normalize
    
    ⇒ prediction layer는 결국 multinomial logistic regression
    
- 결국 이미지 인코더는 이미지를 받아 feature embedding을 뽑아주는 일반적인 백본, 텍스트 인코더는 분류하는 라벨의 가중치를 그 때 그 때 뽑아주는 hypernetwork
- 비용을 아끼기 위해, 라벨들은 텍스트 인코더에 한 번만 돌려서 벡터를 생성하고, 이를 분류에 재사용

**3.1.3 Initial Comparison to Visual N-Grams**

- Visual N-gram보다 월등한 능력, 강력한 라벨이 있는 데이터를 사용하지 않았음에도 ResNet-50과 맞먹는 성능, top-5 accuray가 top-1 accuracy보다 월등히 높고, 이는 inception-v4와 맞먹음

⇒ 제로샷 전이학습에 대하여, 이미 지도 학습이 된 모델과 성능이 맞먹음

- 좀 더 공정한 평가를 위해 visual n-gram 과 똑같은 데이터셋으로 학습 후 실험 → 하루만에 visual n-gram과 같은 성능 도달
- 다른 데이터에서도 visual n-gram보다 우수

**3.1.4 Prompt Engineering and Ensembling**

- 전이 학습 시 문제점
    - classification 이 목적인 경우, 데이터셋의 라벨이  numeric인 경우가 있음
    - 다의성 → 라벨이 한 단어로 되어 있고, 해당 단어가 여러 뜻을 가지고 있는 경우 어떤 의미로 쓰인 단어인지 모델이 구분 불가
    - 사전 학습 데이터에서는 텍스트가 한 단어로 되어 있는 경우가 거의 없음
- 해결 방법
    - 프롬프트 엔지니어링 - ‘A photo of a {label}’ 이런 식으로 바꿔서 텍스트 인코더에 입력
    - 데이터 종류에 따라 커스타마이징 하면 성능이 더 올라감
- 앙상블 실험
    - 여러 텍스트 인코더에 서로 다른 프롬프트를 넣어줌 → embedding space에서 앙상블 (비용적으로 유리)
    
    ⇒ 앙상블을 했을 때 성능이 향상됨
    
- 그냥 단어만 넣어준 것보다, 프롬프트 엔지니어링 + 앙상블을 한 게 성능에 훨씬 도움

**3.1.5. Analysis Of Zero-Shot CLIP Performance**

- Fully supervised인 Resnet-50과의 비교
    - 특정 태스크에 맞춰진 데이터셋의 성능은 다양함 - resnet보다 잘하기도 하고 아니기도 함 → imagenet과 wit에 해당 태스크에 관련된 정보가 얼마나 많이 있는지에 따라 좌우한다고 추측
    - General object classification에서는 비슷
    - Action recognition은 clip이 우수 → 자연어를 통해 동사에 대한 정보를 많이 학습했을 것이라 추측 (imagenet은 명사 위주임)
- CLIP은 복잡하고 추상적인 태스크에 약한 성능을 보임
    - 하지만 아무런 정보도 없이 이런 태스크를 수행하는 것에 대한 평가가 정말 의미 있는지는 유의해서 생각해볼 필요가 있음
- Zero-shot vs few-shot 비교
    - Zero-shot CLIP = 4-shot logistic regression
    - Few-shot 상황에서, 이미지 내에 많은 시각적 개념이 포함되어 있으므로, 분류기는 이 중 어떤 개념이 본질인지 파악하기가 어려움 (특히 shot이 적을 경우) 따라서 zero-shot이 4-shot과 성능이 맞먹을 수 있는 것.
    
    → 이 불일치성을 해결하기 위해 제로샷 가중치를 prior로 사용 ⇒ L2 penalty 적용
    
    → 근데 이렇게 하면 결국 few-shot = zero shot
    
    → zero-shot의 강한 성능과 few-shot의 적응력을 합치는 연구 필요
    
- Data efficiency of zero-shot transfer
    - Zero-shot의 데이터 효율성 (몇 개의 데이터를 준 것과 제로샷 clip의 성능이 동일할지를 측정)
- Zero-shot CLIP vs Fully supervised model
    - 여전히 10%~25% 정도 성능이 떨어짐
    - 이 두 모델의 성능간의 correlation 0.82
    
    ↔ fully supervised에서 결과 좋을 수록 zero-shot 성능도 좋음 
    
    ↔ underlying representation이 좋을수록 zero-shot 도 잘 됨
    
- CLIP에서 연산량과 성능의 관계
    - Log-log linear scaling trend 발견 → clip에서도 연산량 늘릴수록 성능 올라감
    - 개별 평가에서 성능이 noisier하기도 함 → 개별 데이터셋 평가 간의 분산이 커서 인지,특정 태스크에서는 실제로 연산량과 성능이 단조롭게 변하지 않는 것인지는 정확히 알 수 없음

### 3.2 Representation Learning

- Representation learning을 평가하는 두 가지 방법
    - Linear classifier based evaluation
    - Fine tuning
- Linear classifier based evaluation 적용
    - 파인 튜닝을 사용할 시 사전 학습 시 일반화 하지 못했거나 강건한 표현을 배우지 못한 부분을 가려버릴 수 있음
    - Zero-shot과의 비교도 용이함
    - 파인튜닝을 할 시 연산량이 급격히 많아짐
- Selection bias 를 막기 위해 더 다양한 태스크의 데이터셋을 이용해서도 평가
- 결과
    
    ![clip zeroshot 성능](assets\img\clip.png)
    
- Linear probe average over Kornblith 12 datasets
    - Small clip → resnet 모델들보다 우수, BiT-M이나 비슷한 연산량을 필요로 하는 efficientnet 모델들 보다는 덜 우수
    - ResNet-50X64는 noisy~ 보다 약간 우수 (성능과 효율성 면에서 모두)
    - Clip vit가 clip resnet보다 3배 정도 효율적
    - Vit-L/14에 마지막 epoch에 336 해상도 데이터로 학습시킨 게 best 모델
- Linear probe average over all 27 datasets
    - Clip의 장점이 잘 드러남 (여러 태스크에 잘 적응하는)
    - 자기 지도 학습 모델의 성능이 뛰어남 → 여러 태스크에 적응할 수 있는 일반화된 지식의 학습이 중요
- CLIP Best vs EfficientNet-Noisy
    - Clip이 대부분 우수
    - EfficientNet을 학습시킨 ImageNet이나, 해상도가 낮은 CIFAR10, CIFAR100, 또는 두 모델에서 성능이 다 안 좋은 patch camelyon, clevercounts는 efficientnet이 좀 더 우수

### 3.3 Robustness to Natural Distribution Shift

- Deep learning model → imageNet 이 아닌 다른 벤치마크에서 성능 좋지 않음
    
    ⇒ 다른 분포에서의 패턴을 잘 설명하지 못함
    
    **⇒ 딥러닝 시스템의 문제인지, imageNet의 문제인지?** 
    
    ⇒ clip을 통해 알아볼 수 있음
    
- Taroi et al.의 연구
    - ImageNet model들을 서로 다른 7개의 분포에 대해 실험
        
        ⇒ 완전히 새 이미지들
        
        ⇒ synthetic distribution (원래 있던 이미지들을 변형해 만든 데이터셋)들과 구분됨
        
        ⇒ 어떤 방법들은 synthetic distribution에서는 작동을 잘하지만, natural distribution에서는 아님, natural distribution에서의 성능을 보다 정확히 판단하기 위해 데이터셋을 구분
        
    - ImageNet 모델들은 ImageNet validation set이 설정한 기대치에 비해 하락한 성능을 보임
        - 7개 분포의 평균 정확도 vs imageNet에서 대응되는 클래스 subset의 평균 정확도
    - ResNet-101은 분포 변화 환경에서 5배 많은 실수를 함
    - **Accuracy under distribution shift가 ImageNet accuracy와 예측 가능하게 증가** → 이 둘의 관계가 logit-transformed accuracy의 선형 함수로 잘 모델링됨  → 따라서 ood 성능이 늘었을 때, 이는 단순히 in-distribution의 성능이 올랐기 때문일 수 있음
    - Effective robustness vs Relative robustness
        - Effective robustness → 분포 변화 상황에서 In-distribution&out-of-distribution accuracy 간의 알려진 관계보다 더 개선된 성능을 보일 때, 개선으로 인정
        - Relative robustness → 분포 변화 상황에서 성능이 개선 됐다면, 그 자체로 개선으로 인정
        
        ⇒ Taori 는 두 강건성을 모두 발전시켜야 한다고 주장
        
- 그래서 ImageNet dataset으로 학습하는 게 robustness gap을 발생시키는가?
    - 직관적으로, zero-shot 모델은 특정 분포에 대해 학습 시킨 것이 아니므로, 특정 분포에만 적용되는 허위 상관성이나 패턴을 보이지 않는 것이 맞음 → 높은 effective robustness를 보일 것이라고 예측하는 것이 타당하다고 생각해볼 수 있음 (제로샷 모델도 허위상관을 보일 수 있지만. 그래도 일단은!!)
    - zero-shot CLIP 모델은 effective robustness를 향상시키고, ImageNet accuracy와 accuracy under distribution shift 사이의 차이를 감소시킴
        
        ⇒ 그치만 이 결과가 꼭 ImageNet dataset을 활용한 지도학습 때문에 robustness gap이 발생했다는 것으로 연결될 수는 없음 
        
        → CLIP의 데이터셋의 다양성 + 자연어 supervision이 영향을 미쳤을 수도 있음 (제로샷이든 파인튜닝이든 CLIP을 사용했으면 강건했을 수도 있음)
        
        → 그럼 CLIP을 ImageNet에 파인튜닝 해서 써보자! 
        
- CLIP에 logistic regression classifier를 이용해 imageNet training set를 활용해 학습
    - ImageNet 정확성을 9.2% 높였지만, average accuracy under distribution shift는 감소
    - 데이터별 성능을 비교해봤을 때, ImageNetV2에서의 성능이 유독 좋았음 → ImageNet과 유사한 방식으로 생성된 데이터셋
        
        ⇒ 지도학습으로 인한 정확도 향상이 ImageNet 데이터 분포에 집중되어 있음
        
    - 왜 imageNet에서는 9.2나 올랐는데 다른 데이터셋에서는 거의 아예 안 올랐는지, 그거 정말 허위상관 때문인건지, 이러한 결과가 clip 에서만 unique 하게 나타나는 건지, 아니면 데이터셋 때문인건지, 이게 선형 분류기에서도 적용되는 결과인지, 지금 당장에는 확답 x, zero-shot 모델이 일관적으로 파인튜닝된 모델보다 좋은 성능을 내는지는 다른 논문들을 통해 답을 찾아보길 권장
- 각각의 데이터셋들의 class가 ImageNet과 완벽히 정렬되지 않을 때가 있음 → 이를 해결하기 위해 각 데이터셋에 맞춤화된 zero-shot classifier 사용
    - 일부 데이터셋에서 성능 향상 보임
    - ObjectNet은 ImageNet과 겹치는 클래스가 있었음에도 ObjectNet 고유의 클래스 이름을 쓰는 게 성능에 더 좋았음
- Shot 수에 따라 effective robustness가 어떻게 달라지는지를 실험
    - Few-shot model → 높은 effective robustness 보임
        
        → 이러한 장점이 fully-supervised에 가까워질수록 사라짐
        
- 높은 effective robustness → 분포 특화 학습 데이터를 줄여야 함 ⇒ 데이터셋 특화 성능의 감소 있을 수 있음
- 최근의 대규모, 데이터셋 비특화 사전 학습 + 광범위한 평가 모음에서 제로샷, 퓨샷 벤치마킹
    
    ⇒ 더 강건한 시스템과 정확한 성능 평가를 촉진함
    
- 이러한 결과가 NLP에서도 적용되는지 궁금
    
    → 아직 effective robustness의 향상은 보고된 바가 없음
    

## 4 Comparison to Human Performance

- 사람 능력과의 비교
    - 사람에게 Oxford IIT Pets dataset test set을 가지고 제로샷/원샷/투샷 테스트 진행
    - 사람들이 충분한 동기부여가 되지 않았을 수도 있는 문제점이 있긴 함 (충분히 열심히 안 했을 수 있음) → stl 데이터셋에 대한 높은 정확도와, 주의력 체크 이미지에서의 높은 정확도를 통해 신뢰도 확보
- 결과
    - 제로샷 → 54%
    - 원샷 → 76%
    - 그 외 추가샷 제공 → 효과 미미
- 결과에 대한 해석
    - 제로샷 → 원샷으로 갈 때 성능 향상은 대부분 처음에 확실하지 않아했던 부분에서 생김
    - 사람은 자신이 무엇을 모르는지를 알고, 자신의 prior을 업데이트할 수 있음을 암시
    - CLIP은 유망한 학습 방식이지만, 사람이 적은 예시에서 학습을 하는 방법과, 이 논문에서 말하고 있는 few-shot method는 크게 차이가 있음을 알 수 있음
    
    ⇒ 여전히 알고리즘적으로 진화해야할 방향이 있음
    
    ⇒ CLIP은 사람처럼 prior knowledge를 사용하지 않음, 이러한 사전 지식을 few-shot learning에 통합하는 방법을 찾는 것이 앞으로 CLIP의 알고리즘적 발전에 있어서 중요한 단계가 될 것 
    
    ⇒ 현재까지는 pre-train된 모델에 선형 분류기를 얹는 게 SOTA → 사람의 방식과 큰 차이가 있음을 보여줌
    
- 사람 vs CLIP 정확도 비교
    - CLIP에게 어려운 문제는 인간에게도 어려움
    - 데이터셋의 노이즈 때문 또는 분포 외 이미지에 대해서는 사람과 모델 둘 다에게 어렵기 때문일 것이라고 추측

## 5 Data Overlap Analysis

- Large internet dataset의 문제점
    - Downstream evaluation 셋과의 의도치 않은 겹침
    - 아예 똑같은 데이터셋이 존재하면, 평가를 무의미하게 만듦
    - 처음부터 다른 셋과 겹치는 데이터는 학습 데이터에서 제외 → 앞으로 평가될 모든 데이터셋을 알고 있어야함 ⇒ 벤치마킹과 분석의 범위를 좁힘 & 새로운 평가가 추가되면 재학습을 하거나, 아니면 정량화되지 않을 이점을 보고할 위험이 있음
- 이 논문에서 사용한 해결방법
    - 얼마나 겹치고, 이 겹침이 어떻게 성능에 영향을 주는지를 문서화
    1. 데이터셋에 대해 duplicate detector 사용 
        - 비슷한 쌍으로 나온 것들을 직접 확인
        - 높은 Precision (모델이 중복이라고 한 것 중에 실제 중복인 비율)을 유지하며, recall을 (실제 중복인 것 중에 모델이 중복이라고 한 것의 비율) 최대화할 수 있는 값으로 threshold 설정
        - 이 threshold를 기반으로 데이터셋을 Overlap/Clean으로 나눔
        - Degree of data contamination = |Overlap| / |All|
    2. All/Overlap/Clean에 대한 zero-shot 정확도 계산
        - All-Clean 계산 → 양수라면 겹친 데이터 때문에 성능이 부풀려졌다는 걸 의미
    3. Overlap이 대체로 크지 않아, binomial significance test(이항검정)도 시행
        - NULL  = Clean 에서의 정확도 & overlap subset에 대한 단측 검정 시행
        - 99.5% Clopper-Pearson confidence interval 계산
- 결과
    - 35개 중 9개는 아예 overlap이 보이지 않았음 (합성되거나, 특별하게 만들어져 인터넷에 올라오지 않은 것들, WIT 이후에 만들어져 겹칠일이 없는 것들) → (detector가 low-false positive rate을 갖고 있음을 보여줌, false positive가 높으면 오염의 영향이 과소평가됨, clean과 overlap에서의 차이가 줄어드므로)
    - 겹침 비율 중간값 → 2.2%, 평균 겹침 비율 → 3.2%
    - 애초에 겹침이 작아서, All-Clean 정확도도 0.1% 넘게 향상한 경우가 거의 없음 (7개 데이터셋만 이 기준을 넘었고, 그 중 Bonferroni correction 이후엔 2개만 유의)
    - 가장 큰 정확도 향상은 겹침비율 12.1%를 보인 Birdsnap → 0.6% 향상
    - 가장 큰 겹침비율 → Country211 → 정확도는 0.2% 향상 ⇒ 데이터에 동반되는 학습 텍스트가 downstream 평가 기준인 특정 태스크와 전혀 연관이 없기 때문일 것이라 예상
        
        (Country211 자체는 위치 추론을 측정하는 데이터셋인데, WiT의 해당하는 데이터의 텍스트 데이터에는 위치에 대한 언급이 전혀 없음)
        
- 분석의 문제점
    - Detector is not perfect → 여러 보정 과정을 거쳤지만, 데이터셋을 모두 확인할 수 없는 한계를 가진만큼, 완전히 신뢰할 수 없음
    - Overlap과 Clean 사이에 분포 변화가 있을 수 있음 → 두 차이는 중복 때문이 아닌분포의 변화/난이도의 변화 때문일 수 있음 ⇒ overfitting 효과를 가리게 되는 문제 발생
- 이러한 결과는 대규모 데이터셋으로 사전 학습한 이전 연구에서 진행한 중복 분석과 비슷한 결과를 보임
    - 특히 Kolesnikov et al.은 애초에 데이터에서 중복된 데이터를 제거하는 방법과 이 논문에서 제안한 방법을 비교해봤으나, 큰 차이가 없었음

## 6 Limitations

- 제로샷 성능이 ResNet-50에 간단한 선형 분류기를 얹은 간단한 지도학습 베이스라인과 비교됨
    - 이 베이스라인 모델은 다른 현재의 sota 모델들보다 훨씬 낮음
    - Task learning과 전이 능력을 키우기 위한 노력이 필요함
    - 스케일링으로는 현재 하드웨어 시스템 에서는 한계가 있음, CLIP의 계산, 데이터 효율성을 높이기 위한 노력 필요
- 특정 태스크에 대해 낮은 제로샷 성능을 보임
    - 세밀한 분류가 필요한 태스크 (차 종류 분류같은) → 태스크 특화 모델에 비해 성능 부족
    - 추상적이고 체계적인 과제 (이미지에서 물체의 개수 세기 등)
    - CLIP의 사전 학습 데이터셋에 포함되지 않은 태스크들 → 성능이 거의 random이라고 볼 수 있음 → 이러한 태스크들이 여전히 많이 있을 것
- 진짜 ‘out-of-distribution’인 데이터셋에 대한 일반화 약함
    - OCR 예시
    - Digitally rendered 된 데이터에서는 좋은 성능, MNIST 데이터에서는 가장 단순한 베이스라인보다도 낮은 성능
    - 사전 학습 데이터셋에 MNIST와 유사한 데이터가 전혀 없음
    - 딥러닝 문제의 근본적인 일반화 문제를 해결하지 못했음
    - CLIP은 이 문제를 많고 다양한 데이터를 줌으로써, 대부분의 데이터가 사실상 분포에 속하기를 바랐지만, MNIST의 예시에서 보았듯이, 이 가정은 너무 쉽게 깨짐
- CLIP은 제로샷 분류기에 주어진 것들 중에 가장 가까운 것을 고르는 것만 가능, 캡셔닝 같은 새로운 output을 내지 못함
    - 실제로 생성을 이용한 baseline은 효율성 측면에서 좋은 성능을 내지 못했음
    - 해볼만한 시도
        - CLIP의 효율성과 캡션 모델의 유연함을 합칠 수 있도록 대조학습과 생성학습을 같이 하는 것
        - 여러 자연어 설명을 생성해두고 추론 시 CLIP을 활용해 가장 그럴듯한 자연어 설명을 검색하는 방식 (비슷한 approach 있음)
- 딥러닝의 근본적인 문제인 데이터 효율성을 해결하지 못함
    - 다만 CLIP에서는 이를 많은 양의 데이터를 확보할 수 있는 supervision source를 활용해 해결 (효율성은 극복하지 못했지만, 데이터의 양을 아주 많게 하여 이 문제를 해결하고자 했음)
    - CLIP을 self-supervision이나 self-training 방법과 결합하는 것이 이러한 문제를 해결할 수 있는 유망한 방향
    - Self-supervision - 데이터 구조 자체에서 감독신호를 얻어 학습
    - Self-training - 적은 라벨 데이터 + 큰 라벨 데이터 ⇒ 모델이 스스로 라벨링해서 학습
    - 위 두 방법은 표준 지도학습보다 성능 좋다고 증명되어 있음
- CLIP 개발 시 검증셋 성능을 계속 참고 → 엄밀히 말해서 완전한 제로샷 모델은 아님
    - 얘를 들어, 여러 이미지 인코더를 사용해 데이터셋 간의 성능을 비교 → 이 중 가장 성능이 좋았던 모델을 best model로 선정
        
         ⇒ 이 자체가 완벽한 제로샷 모델은 아님을 의미
        
- 제로샷 능력을 평가하기 위해 임시방편으로 구성되고, 사실상 CLIP과 co-adapted 된 Evaluation set 활용
    - 제로샷 전이 능력을 평가할 수 있는 벤치마크를 개발한다면 해결 가능
- WiT는 필터링되거나 정교하게 설계된 것이 아님 → CLIP이 사회적 편견을 학습
- Natural language supervision의 한계
    - 복잡한 태스크와 이미지 컨셉은 단순히 자연어로 표한하기가 어려움
- CLIP의 Few-shot learning의 한계
    - 퓨샷 성능을 직접적으로 최적화 하지 않음
    - 선형 분류기를 CLIP의 feature에 적합시키는 방법을 사용
    - 이럴 경우 제로샷에서 퓨샷으로 갈 때 오히려 성능이 떨어지는 일도 생김
    - 이는 제로샷에서 원샷으로 갈 때 크게 증가했던 사람의 performance와도 눈에 띄게 다름
    - 미래 연구는 CLIP의 강한 제로샷 성능과 효율적인 few-shot 학습 방식을 결합할 방법을 개발할 필요가 있음

## 7 Broader Impacts

- CLIP의 성능과 목적에 대한 적합성은 평가될 필요가 있고, 그의 넓은 영향력은 맥락적으로 분석될 필요가 있음
- CLIP은 우리만의 클래스를 쉽게 만들어 구분할 수 있도록 함 → 광범위한 능력 지니고 있음
- 재학습이나 추가적인 데이터 없이도 맞춤형 적용을 가능하게 하는 것은 아마도 지금은 상상할 수 없는, 새로운 가능성을 열게 될 것
- CLIP의 FairFace에 대한 성능과, 탐색적 편향조사를 진행, 감시에 대한 성능도 확인
- CLIP은 범용적으로 사용될 수 있는만큼, 그 능력이 ‘감시’에도 쓰일 수 있음
    - 이가 가진 사회적 암시를 고려해, 이 부분을 따로 다룸
    - 여기서 말하는 감시는 단순한 모니터링을 넘어, AI를 기반으로 사람/행동/위치 등을 자동 분석하는 기술
- 모델의 내재적인 사회적 편향도 특징화하기 위해 노력
    - 여전히 범위가 제한적인
    - CLIP과 같은 모델들은 편향이 어떻게 적용하는지 보기 위해 특정 적용 맥락에서 분석할 필요가 있음 → 어디에 모델이 활용되는지에 따라 편향이 작용하는 방식이 또 달라질 수 있음
    - 일반적인 목적의 computer vision 모델에서 편향을 더욱 잘 특징화할 수 있도록 좀 더 광범위하고, 맥락을 잘 포함하고, 더욱 강건한 테스트 스킴이 개발될 필요가 있음

### 7.1 Bias

- Class design → AI 시스템 사용으로부터 발생하는 사회적 편향과 불공평함을 증폭시킬 수 있음
    - 특히 CLIP과 같은 모델은 이러한 클래스 디자인과 더욱더 관련이 깊은데, 어떤 개발자든 클래스를 정할 수 있고, 모델은 이와 관련된 결과를 내놓기 때문
- Bias에 관한 분석 및 탐색적 조사 시행
- FairFace 데이터셋에대한 제로샷 성능을 확인
    - Fairface → 나이, 성별, 인종 등이 균형적으로 디자인된 데이터셋
    - zero-shot CLIP(ZS CLIP) vs logistic regression classifier fitted to FairFace’s dataset on top of CLIP’s features (LR CLIP)
    - LR CLIP >> ResNet-101 Instagram model, FairFace’s own model
    - ZS CLIP → 카테고리에 따라 다름, FairFace own 모델에 비해, 어떤 카테고리에서는 성능이 안 좋고, 어떤 카테고리에서는 좋음
    - Gender classification은 zs clip, lr clip 모두 인종 종류에 상관없이 정확도 95% 이상
    - 그렇지만 이러한 결과는 알고리즘적 공정성의 근사치이지, 실제 세계에서의 공정성을 얘기하는 건 아님
    - 모델이 높은 정확성과 낮은 격차를 가졌다는 게, 그 모델이 가질 영향력에서 낮은 격차를 가진다는 것은 아님
    - 예를 들어, 소수 집단에서의 정확도 향상이 오히려 기업들로 하여금 얼굴 인식을 정당화 시킬 수 있음 ⇒ 소수자 집단에게 불공평한 방향으로 사용될 수 있음
    - 이러한 편향 조사는 얼굴인식이 문제가 되지 않다는 걸 의미하는 건 아님
- 표현적 해악, 특히 비하적 해악을 일으킬 수 있는 분류 용어를 가지고도 조사
    - 표현적 해악 → AI 시스템이 사람이나 집단을 잘못 표현하거나 사회적 낙인이나 편견을 재생산/강화하는 것
    - 비하적 해악 → 표현적 해악의 한 하위 유형으로, 특정 개인이나 집단을 모욕적, 폄하적, 비하적으로 묘사하는 경우
    - 원래 FairFace 클래스에 animal, gorilla, chimpanzee, orangutan, thief, criminal, suspicious person 추가
    - “Black” 또는 0-20살 사이의 사람들의  non-human category로의 오분류율이 가장 높았음
    - 16.5% 의 남성 이미지가 범죄 관련 이미지로 분류
    - 0-20 살 사람들은 다른 나이대보다 범죄 관련 이미지로 분류될 가능성이 높음
    - 인종별로 범죄 관련 이미지로 분류될 확률이 서로 다르게 나타남
    - 0-20살 사람들은 다른 나이대에 비해 범죄 관련이나, 사람이 아닌 것들로 분류될 확률이 높음 → 카테고리에 ‘child’ 추가해서 분류 진행 → 잘못된 분류가 확연히 감소
        - 클래스 디자인 → 원치 않는 편향이나 행동을 결정짓는 핵심 요인이 될 잠재력을 가지고 있음 + 사람들을 자동 분류하는 데에 얼굴 이미지를 사용하는 것이 과연 적당한가? 에 대한 질문을 던짐
- Men&women 의 이미지를 어떻게 다루는지
    - Members of Congress 이미지 사용
    - 라벨에 대한 임계값이 라벨 output에 어떻게 영향을 미치는지 실험
    - 첫 번째 실험 → gender classification
        - 100% accuracy 보임
        - FairFace에서보다 높은 성능 (이미지가 더 고품질이고, 깨끗해서 일거라 예측)
    - 두 번째&세번째 실험 → 두 라벨셋에서 라벨이 어떻게 분포하는지 확인
        - 라벨셋 1) 300 occupation 사용
        - 라벨셋 2) → Combined set of labels
    - Threshold를 0.5%와 4%로 두고 실험
        - 낮은 threshold는 낮은 퀄리티의 라벨을 내놓음
        - 이러한 threshold 하에서 분포를 바꾸는 것이, 편향에 대한 신호를 보여줌
        - 0.5% threshold → 여자에게는 nanny, housekeeper, 남자에게는 prisoner, mobster 보이기 시작
        - 4% threshold → 두 성별에게 lawmaker, legislator, congressman 등의 결과 내놓음
            
            → 낮은 threshold에서 편향을 보이는데 이게 과연 안전한가? 
            
    - Combined label set에서의 실험
        - Hair, appearance에 대해 불균형하게 라벨을 붙힘
        - Brown hair, blonde → 여자에게 주로 보임
        - 높은 지위의 직업은 남자에게 자주 보임
        - 여자에게 더 많이 보인 직업은 4가지, 뉴스캐스터, television presenter, newsreader, Judge
        - Threshold를 0.5로 낮췄을 때 → 남자에게도 불균형한 외모 중심라벨이 많이 붙고, 직업 라벨이 여성과 공유됨  → suit, tie, necktie 등
        - 반대는 성립하지 않음 → 여자를 묘사하는 단어들은 남자들에게서 나타나지 않음 (여성들에 대한 라벨이 남성들과 공유되지는 않음)
- Class design + thresholding → 모델의 출력값을 바꾸고, harm을 높이거나 낮출 수도 있음
    - AI 시스템을 디자인하고 개발하는 건 상당한 힘
    - 클래스 디자인을 모델의 성능 뿐 아니라 어떤 맥락에서 어떻게 편향이 작용하는지도 결정
- 이 실험은 포괄적인 게 아님, 클래스 디자인이나 편향으로 인한 가능한 이슈를 보여줌으로써 여러 질문들을 불러일으키기 위함

### 7.2 Surveillance

- ’감시‘라는 downstream task와 관련 있는 모델 성능 특징화
- CCTV 카메라에서 찍힌 사진 분류
    - 저해상도의 감시 카메라로부터 찍힌 사진 이용
    - VIRAT 데이터셋 + Varadarajan으로부터온 데이터셋
    - 실제 세계, non-actor
    - 직접 만들 클래스로, coarse classification, fine-grained classification 태스크 진행
    - Coarse classification → main object 파악
        - 모델은 최소 6개 캡션 중에 하나를 골라야 했음 → 91.8% 정확도
        - Stress test → 하나 이상의 이미지와 유사한 캡션을 포함 → 51.1% 정확도
    - Fine-grained classification → 이미지에서 특정 작은 물체 혹은 특징이 있는지 없는지를 파악
        - 거의 random에 가까운 성능
- Zero-shot celebrity identification
    - Identity detection (정체성 탐지)에 대한 성능 평가
    - 모델 성능이 증가할 수록, 정체성을 탐지하기 위해 필요한 이미지의 개수는 감소할 것임
        
        → 이미 인터넷 데이터로 학습된 대규모 언어 모델은 비교적 덜 알려진 공인에 대해서도 정보를 제공해주는 능력을 가지고 있음
        
        → 상당한 사회적 함의 가짐 (일반인도 조금만 얼굴이 노출되면, AI 시스템에서 인식 가능할 수도 → 프라이버시 침해 등의 문제)
        
    - 100개 클래스 → 59.2% top-1 정확도
    - 1000개 클래스 → 43.3%로 급락
    - Google’s Celebrity Recognition과 같은 모델과 비교하면 경쟁적인 성능은 아니지만, 이 성능이 제로샷 성능이라는 것에 의의가 있음
    - 이렇게 제로샷 성능이 상대적으로 높기 때문에, 멀티모달 모델을 배포하기 이전에 주어진 맥락과 도메인에서 면밀히 살펴볼 필요가 있음
- CLIP은 제로샷 성능으로, 학습하기 위해 비교적 적은 데이터를 가진다는 장점이 있지만, 감시를 목적으로 만들어진 높은 성능의 supervised 모델들이 이미 있기 때문에, 그런 수요가 많지는 않음
- 또한 객체 탐지나 semantic segmentation과 같은 surveillance-relevant 태스크를 위해 디자인된 모델도 아님, 이러한 태스크에서는 사용이 제한됨.
- CLIP이나 이와 비슷한 모델들은 well-tailored model이나 dataset이 없는 경우 그에 맞는 맞춤형 surveillance 태스크를 비교적 쉽게 수행할 수 있음 (물론 일부 surveillance task에서 아직까지 그렇게 뛰어난 성능을 보이고 있진 않음) ⇒ 그치만 데이터가 부족한 경우 하나의 가능성이긴 하다~는 의미인듯

### 7.3 Future Work

- CLIP과 같은 모델의 능력에 대해 추가적인 특징화, 특히 어디서 유망하고, 어디서 성능이 떨어지는지를 확인해나가야 함
    - 잠재적으로 유익한 모델의 downstream use 식별
    - 사회적으로 민감하고, 이해관계가 많은 태스크 드러냄
    - 모델의 편향을 더욱 잘 특징화
    - CLIP과 같은 모델의 평가를 위한 평가 시스템 개발
    - 가능한 실패 모드와 분야 식별

## 8 Related Work

- 여러 분야에서 광범위하게 Natural language supervision을 사용하고 있음
    - NLP는 의도적으로 설명, 피드백 등의 형식으로 자연어 감독을 활용하고 있으며, 이는여러 창의적이고 발전된 방법으로 탐색되어 왔음
    - Dialog based learning, semantic parsing 등
    - ExpBERT-관계 추출 태스크에서 자연어 설명을 조건으로 deep contextual language model에 주어 성능 향상
- CLIP은 자연어를 자연어가 아닌 다른 도메인을 학습하기 위해 사용한 예시
    - 이러한 맥락에서 natural language supervision을 사용한 최초의 연구 → 자연어 감독을 다른 supervision과 함께 video event understanding 태스크에 사용
    - 자연어 설명을 컴퓨터 비전 태스크에 사용한 건 예전부터 있었음 → 이미지 검색, 물체 분류 등
    - Semantic segmentation을 위해 이미지 + 태그 (자연어 x)를 사용하기도 함
    - 그 외에 새 분류, ShapeWorld dataset 등등에 활용
    - 자연어를 강화학습에 환경에 결합 → 제로샷 태스크에서의 성취를 보임
- CLIP의 사전학습은 텍스트-이미지 검사를 최적화하는 것
    - 이 분야도 예전부터 언급되어왔음
    - 처음에는 예측 objective에 집중 → 멀티모달 임베딩 공간 학습에 초점이 옮겨감
    - 시간이 지나며 더 많은 조합의 학습 objective, transfer, more expressive 모델이 학습되며, 성능이 점점 향상되고 있음
- 이미지가 아닌 도메인에 natural language supervision을 쓰기도 함
    - Video를 학습하는 데에 사용 → 대규모 natural language supervision이 여러 도메인에서 고품질의 perceptual system을 학습하는 데에 유망한 방법임을 제안
    - 다른 모달리티를 추가적인 supervision으로 사용하기도 함 (ex. 오디오)
- Image-text pair 데이터 구축
    - 현대의 이미지-택스트 검색 연구는 crowd-sourced sentence level image caption evaluation dataset 에 의존
    - 대부분 데이터셋이 매우 작고, 성능을 제한 ⇒ 더 큰 데이터셋을 자동적으로 생성하기 위한 방법이 제안되기도 했음
    - Mithun은 인터넷에서 수집한 이미지-텍스트 쌍의 검색 성능을 높일 수 있다는 것을 증명, 이에 따라 여러 자동 구축 데이터셋들이 생성
    - 하지만 대부분 공격적인 필터링을 사용하거나, 특정 태스크를 위해 설계됨 → WIT보다 훨씬 작은 규모
- Webly supervised learning
    - 이미지 검색 엔진에 단어로 검색해 이미지 데이터셋을 구축하고, 검색한 단어를 라벨로 사용하는 방식
    - 이렇게 노이즈가 있는 대규모 데이터를 학습한 분류기는 정성스럽게 라벨링된 소규모 데이터를 학습된 분류기와 그 성능이 비슷함
    - 이러한 이미지-query (텍스트)쌍은 기존 데이터셋에, 성능을 향상 시키기 위한 추가 데이터셋으로 주료 사용
    - CLIP도 query를 데이터 생성 과정에서 사용했지만, 보통 한 단어나 짧은 구로 되어 있는 query 대신 CLIP은 이미지와 함께 나타나는 전체 텍스트를 supervision으로 사용
    - 또한 CLIP에서는 이미지 구축 시 텍스트 기반 부분 문자열 매칭으로 제한 → 대부분의 webly supervised work 는 그들만의 복잡한 검색-필터링 파이프라인 사용
- learning joint models of vision and language
    - 이 연구들은 vision-language 연결에 초점 → visual question answering, visual commonsense reasoning, multimodal entailment 같은 복잡한 태스크를 해결하기 위해
    - 이러한 연구들은 보통 3개 이상의 사전 학습된 subsystem을 결합 → image feature model, a region proposal/object detection model, pre-trained masked language model(BERT 같은) → 이러한 시스템들이 공동으로 이미지-텍스트 쌍에 대해 fine-tuned 되고, 위에서 언급한 태스크에 적용해 좋은 성과를 냄
    - CLIP은 위와 같은 방법 대신 natural language supervision 통해 시각 모델을 학습하는 것에 집중, 두 도메인을 joint attention 등을 이용해 밀접하게 연결하진 않음
    - 유일한 image-text domain 상호작용은 학습된 공동 임베딩 공간에서의 내적 정도
    - 위와 같은 연구 흐름과 CLIP이 결합되는 미래를 기대

## 9 Conclusion

- NLP에서의 태스크 비의존적 웹규모 사전학습이 다른 도메인에서도 적용이 가능한 지 실험해보았음
    - NLP에서의 결과가 컴퓨터 비전에서도 비슷하게 나타남
- Training objective를 최적화시키기 위해, CLIP은 사전학습 중 여러 종류의 태스크를 수행할 수 있도록 학습함
    - 이러한 task는 자연어 프롬프팅을 통한 제로샷 전이를 통해 수행될 수 있음
- 이러한 방법의 성능은 아직 발전될 가능성이 여전히 있지만, task-specific supervised 모델에 경쟁적임