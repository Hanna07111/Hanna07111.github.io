---
title: "Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation"
date: 2025-10-20
categories: [Capstone, 논문리뷰]
tags: [Knowledge Distillation, CLIP, Meta Distillation]
---

[Distill CLIP (DCLIP): Enhancing Image-Text Retrieval via Cross-Modal Transformer Distillation](https://arxiv.org/abs/2505.21549)

## Abstract

- fine-tuned variant of CLIP model
    - multimodal image-text retrieval 능력 향상 + 제로샷 분류 능력 유지
- 기존 clip의 한계
    - 고정된 image resolution, 제한된 문맥으로 retrieval task에서 효과적이지 못했음
- DCLIP의 해결책
    - meta teacher-student distillation framework 사용
    - cross-modal transformer teacher →  produce enriched embeddings
    - 이 때 transformer 파인튜닝은 yolo 추출 이미지+대응되는 텍스트 span(?)의 양방향 cross attention 사용 →????
- 적은수의 샘플로 학습시켜도, DCLIP은 image-text retrieval metircs에서 높은 향상을 보임 + 기존 clip의 제로샷 분류 성능도 거의 유지
    
    ⇒ task specialization과 generalization 사이의 트레이드 오프를 효과적으로 완화
    

## 1 Introduction

- CLIP의 단점
    - 대규모 noisy pair로 학습 ⇒ fine-grained(세밀한)
    
    ⇒ 이해를 제한, 객체의 속성을 정확히 찾아내거나, 객체 간의 복잡한 관계를 모델링 하지 못함 
    
    ⇒ 오픈 도메인 검색이나, compositional reasoning에 취약
    
- 이를 해결하기 위한 시도/모델
    - dense grounding, prompt engineering, architectural modification
    - LongCLIP, TULIP → clip의 제한된 input은 완화
    
    ⇒ 효율적인 세밀한 검색 + 강한 일반화 능력 유지 
    
- DCLIP
    - fine-grained retrieval 능력 향상+제로샷 능력 유지
    - meta-teacher가 region-level attention으로 이미지 임베딩 enrich
    - student model은 이미지 인코더가 meta-teacher의 output을 모방하도록 파인튜닝
    - student model의 텍스트 인코더는 frozen
    - YOLO로 coarse region 추출 + 양방향 cross-modal attention
        
        →  fine-grained embedding 생성 → student 모델의 distillation target으로 사용
        
    
    ⇒ region processing이나, 일반화 능력을 떨어뜨리는 파인튜닝 없이도 향상된 표현을 배울 수 있음
    
- DCLIP의 성능
    - text-to-image retreival → Recall@1 20% 넘게 향상
    - CLIP의 제로샷 능력 94% 유지
    - 더 큰 모델로 scaling도 가능 →?
- 이 논문의 주요 contribution
    - lightweight distillation framework
    - asymmetric student-teacher architecture
    - empirical results showing strong retrieval gains with limited data and compute

## 2 Related Work

### 2.1 Contrastive Vision-Language Models

- contrastive vision-language pretraining → **CLIP**, ALIGN 등
- 강한 제로샷 일반화 성능
- limitations
    - **fine-grained understanding → DCLIP에서 중시하는 부분**
    - handling variable text length

### 2.2 Region-Aware Vision-Language Understanding

- detailed visual semantic을 포착하기 위해 많은 논문에서 region-level processing 사용
    - YOLO, Faster R-CNN 사용
- DCLIP → YOLOv8 사용
    - explicit region processing은 교사에만 사용
    - 학생은 증류를 통해 세밀한 표현을(fine-grained understanding) 내제적으로 배우게 됨

### 2.3 Knowledge Distillation for Vision-Language Models

- larger model에서 smaller model로의 지식 전이 기법
- DCLIP
    - 교사 → region-enhanced representation으로 파인튜닝
    - 학생 → 교사로부터 richer semnatic 학습 (contrastive learning + direct feature distillation)

### 2.4 Meta Distillation

- 단순히 pre-training된 모델이 아닌 더 최적화되고 향상된 교사 모델을 사용
    
    → 학생 모델에 전달되는 지식의 질을 높이기 위함
    
- meta distillation strategy
    - teacher assistant
    - iterative self-refinement 등등..
- KD는 교사-학생의 공동 최적화 ⇒ 교사의 특성과 학습과정이 매우 중요
    - DCLIP 또한 이를 반영 → teacher 모델을 파인튜닝
    - 양방향 cross-modal attention + YOLO-extracted region feature 사용
    - 이를 통해 더욱 fine-grained embedding 생성

## 3 Methodology and Architecture

- DCLIP architecture
    - teacher-student framework

### 3.1 Cross-Modal Teacher Design

- teacher model
    - YOLOv8x - bounding box 추출
    - 각 region은 YOLO의 classification confidence, bounding box area, 텍스트와의 코사인 유사도를 기준으로 가중치가 부여됨
    - bidirectional cross modal attention → text, region embedding 결합
        
        → sementically rich representation produce
        
- bidirectional cross-modal attention
    - two separate fine-tuneable multi-head attention layer로 이루어짐
        - text attends to YOLO-derived image regions 
        (어떤 텍스트 토큰이 어떤 이미지 영역을 더 주목해야하는지를 계산)
        - image regions attend to the corresponding text 
        (특정 이미지가 어떤 텍스트 토큰을 주목해야하는지를 계산)
        
        ⇒ 이렇게 대칭적인 구조가 두 모달리티가 더 정렬되고 문맥적으로 풍부한 임베딩을 배울 수 있게함

<details markdown="1">
  <summary>gpt 질문</summary>

    - 그럼 어텐션이 계산될 때 문장의 텍스트 토큰 1의 임베딩 <-> 이미지 영역 1의 임베딩 문장의 텍스트 토큰 2의 임베딩 <-> 이미지 영역 1의 임베딩 문장의 텍스트 토큰 1의 임베딩 <-> 이미지 영역 2의 임베딩 문장의 텍스트 토큰 2의 임베딩 <-> 이미지 영역 2의 임베딩 이렇게 계산되는 거야? ⇒ **맞음**
    - 근데 그러면 지금 text input으로 문장이 들어가잖아, 어텐션은 텍스트 토큰별로 되는 게 맞아? ⇒ **맞음**
    - 이미지 임베딩만 전달하는데 bidirectional attention을 계산하는 이유?
        - **Teacher 임베딩이 더 풍부해짐**
            - “Image → Text”만 있을 때보다,
                
                “Text → Image”가 추가되면 텍스트 문맥이 한 번 더 강화돼서
                
                이미지 임베딩이 더 안정적·세밀해진다.
                
        - **양방향 retrieval 성능 향상**
            - DCLIP은 T→I뿐 아니라 I→T 성능도 향상시키길 원했어.
                
                → Text→Image direction만 쓰면 T→I는 좋아지지만, I→T는 상대적으로 약해짐.
                
                → 양방향 attention을 쓰면 두 방향이 균형 있게 좋아짐.
                
        - **학습 안정성 (gradient symmetry)**
            - 두 attention 모듈이 대칭 구조라서 loss와 gradient 흐름이 안정적임.
                
                → teacher가 학습될 때 과도한 모달 편향을 방지.
                
    - 근데 그러면 image->text 방향의 attention에서 사용하는 text는 '이미지'가 이미 반영된 정보인건가? ⇒ **아님, 병렬적으로 진행**
</details>

- Attention 이후
    - Temperature scaled attention을 활용해 합쳐짐 → global attention
    - Z - global attention
    
    ![image.png](assets\img\dclip.png)
    
- Cross attention의 단점(?)
    - Teacher 모델을 바로 사용하게 되면
        
        → text와 이미지를 무조건 동시에 넣어줘야 함(attention을 계산하기 위해서)
        
        ⇒ clip의 제로샷 일반화 능력, 단일모달 사용성을 해침
        
        ⇒ teacher의 능력을 CLIP student에 증류 (단일 모달을 인풋으로 받는)
        
        ⇒ 학생의 representation을 선생의 representation에 맞춰 정렬
        
        ⇒ 선생의 fine-grained alignment를 배움 + clip 모델의 유연성, 제로샷 능력 유지
        
        ⇒ 바운딩 박스를 보지 않고도 YOLO의 시각을 배울 수 있게 됨
        

### 3.2 Student and Teacher Distillation Losses

- teacher/student 훈련
    - Contrastive loss → negative pair 멀어지게
    - Cosine loss → student가 teacher의 detail한 embedding을 배우게
- Teacher Loss
    - Cross-attended image embedding ↔ frozen CLIP text embedding 간의 contrastive loss
    
    ⇒ 이걸로 cross-modal attention 파인튜닝
    
- Student Loss
    - 일반화 능력은 유지하면서 Retrieval-optimized 임베딩을 구축하는 것이 목표
    - Cosine distillation+contrastive loss 사용
    - Contrastive loss → task-specific한 structure, teacher의 richer representation을 배우게 함
    - Contrastive loss → teacher embedding에 overfit 되지 않고, original clip structure와 generalization 능력을 어느정도 유지하게끔 해줌

## 4 Experimental Set Up

- Larger student model의 문제점 (ex.ViT-L/14)
    - 기본 distillation → 텍스트에 과하게 맞춰지는 경향이 생김 ⇒ 텍스트-이미지 균형 깨짐
    
    ⇒ embedding aggregation strategy 수정
    
- Embedding aggregation strategy
    - Multi-cluster aggregation
    - 세 개의 semantic cluster에서의 임베딩을 평균 내는 것이 훨씬 robust한 결과를 가져옴
- 그 외 수정사항
    - CLIP의 default absolute positional embedding 대신 ROPE 사용
    - 원본 CLIP ViT-L 이미지 임베딩과 student 이미지 임베딩 간의 Simple cosine distillation preservation loss도 추가

## 5 Results

### 5.1 Evaluation Metrics

- 데이터셋
    - Karpathy dataset (MSCOCO+FLICKR30k)
- 제로샷 성능 측정
    - ImageNet
    - CIFAR-10, CIFAR-100
- Metric for Retrieval
    - Recall@K
    - Mean Average Precision(MAP)
- Metric for Zero-shot Classification
    - Top-1&Top5 accuracy

### 5.2 Experimental Results

- DCLIP의 aysymmetric한 구조
    - Text embedding에 anchor을 두고 image encoder를 학습
        
        → text-to-image에 강점
        
- ViT-B/16 → best trade-off
    - Boosting both retrieval direction
    - Retain about 94% of classification accuracy
- DCLIP의 장점
    - 다른 approach 보다 high-capacity backbone으로의 스케일이 자연스러움
    - Minimal overhead+straightforward aggregation strategy
- epoch과 제로샷 능력의 관계
    - 학습시키는 에폭 수가 많아질수록, 제로샷 능력은 하락
    - 학습을 오래 시킬수록 clip의 원래 임베딩 스페이스에서 멀어지지만, retrieval 성능은 올라감 (text-to-image 한정) (specialization에는 강해지고, generalization은 약해진다)
    - 에폭 수를 늘릴 수록 Text to image retrieval 능력은 향상, 반대 방향(Image to Text)은 살짝 하락하기까지함
- Teacher의 convergence 여부
    - 에폭 2-3으로 학습시킨 teacher을 활용하는게 validation loss 는 커도 retreival-zeroshot 간의 균형이 제일 잘 맞음
        
        ⇒ teacher가 converge 해야한다는 기존 생각에 반하는 지점
        
        ⇒ 이 균형점을 ‘sweat spot’ 이라고 부름
        
- ViT-L
    - ViT-L은 오버피팅의 위험+제로샷, 일반화 성능 희생이 너무 큼 (그 동안의 증류 논문에서 ViT-L을 잘 쓰지 않은 이유기도 함)
    - 일반화 능력보다 Retrieval 성능을 최대화 시키고 싶을 때 보통 사용됨

## 6 Ablation Testing and Results

- No Region-Level Embeddings
    - YOLO를 통해 더욱 이미지적 정보를 많이 얻고 사용할 수 있음
    - YOLO를 없앰 → 제로샷 능력이 향상
- No Bidirectional Cross-Attention
    - Bidirectional cross-attention 대신 simple average of patch and text embedding 사용 (패치 임베딩이미지와 텍스트 임베딩을 간단히 평균)
    - global representation을 위한 original temperature-scaled attention은 유지
    - 그냥 Clip보다는 낫지만, 최적은 아님
    - 모달리티 간의 인터랙션이 중요함을 보여줌
- No Meta-Teacher Guidance
    - ViT-B/16 → teacher가 없으면 성능 하락
    - ViT-L/14 → teacher에 오버피팅 → 일반화 성능 하락
    - Distillation은 learning capacity와 inductive bias가 external supervision으로부터 보완될 수 있는 compact한 모델에 효과적
- No CMA → no bidir attention과의 차이점
    - Cma를 없앤다 → 텍스트↔ 이미지 간 상호작용을 아예 고려하지 x
    - No bidir attention은 두 모달리티 간 상호작용은 적용하되 양방향으로 하지 않는 것 (그냥 평균만 낸 것)
- ViT-L/14 (No Teacher)
    - Teacher가 없을 때 더 retrieval performance ↔ generalization 균형적임
    - Teacher가 없을 때 text-to-image 성능 향상 (대신 image-to-text 성능과 전체적인 임베딩 균형이 하락)
    
    ⇒ 큰 ViT 모델은 오버피팅을 피하게 위해 structured한 supervision이 필요
    

## 7 Limitations

- Yolo를 사용하는 구조의 한계
    - Yolo에 의존하는 구조가 됨 + object annotation이나 믿을만한 detector가 없을 때 유연성 떨어짐
    - 특정 도메인에 최적화되어 있음
    - 다른 도메인에 쓰기 위해서는 또다른 어노테이션 또는 다른 region proposal method를 사용해야 할 수도 있음
- 큰 모델을 증류하는 것의 어려움
    - 오버피팅되기 쉬워서
    - 큰 모델을 teacher처럼 fine-grained 하게 학습시키되, 일반화를 유지할수 있는 방법을 찾는 게 open challenge
- 컴퓨터 자원의 한계로 상대적으로 작은 규모의 데이터셋으로 학습+평가됨
    - 큰 데이터셋에서의 성능은 open question

## 8 Conclusion

- Student model 에 spatial grounding을 통합하는 것이 효과적임을 보여줌
- Spatial+semantic knowledge를 distill하는 것이 lightweight하지만 competitive한 real-world deployment의 대체 방안이 될 수 있음

---

- Text-to-image에 강점 → 우리가 image-to-text에 강점을 가지게 할 수 없을까????
- 큰 모델은 증류하기 어렵다는 내용이 clip-kd에 이어 또 나옴 → 지식 응축 레이어 사용? (근데 이유가 좀 다름.. clip-kd는 교사-학생 크기 차이, 여기서는 학생 모델의 오버피팅)
- DClLP을 제로샷 retrieval을 위한 방법으로 확장시켜볼 수 있나?